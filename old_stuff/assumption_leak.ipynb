{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers will pass the answer leaking test. \n",
    "Let's say there exists a column in the dataset that is the target moved back a time step so that it leaks the answer. Can the transformer based model find and us this information? This is literally a causality not a correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import dotdict\n",
    "from exp.exp_informer import Exp_Informer\n",
    "import torch\n",
    "from utils.ipynb_helpers import setting_from_args, read_data, write_df, handle_gpu\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.model = \"informer\"  # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "\n",
    "args.data = \"custom\"  # data\n",
    "args.root_path = \"./data/stock/\"  # root path of data file\n",
    "\n",
    "\n",
    "args.data_path = \"close.csv\"  # data file\n",
    "args.features = \"MS\"  # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = \"XOM_close\"  # target feature in S or MS task\n",
    "args.freq = \"t\"  # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.checkpoints = \"./checkpoints\"  # location of model checkpoints\n",
    "\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "args.c_out = 1  # output size\n",
    "args.factor = 5  # probsparse attn factor\n",
    "args.d_model = 512  # dimension of model\n",
    "args.n_heads = 8  # num of heads\n",
    "args.e_layers = 2  # num of encoder layers\n",
    "args.d_layers = 1  # num of decoder layers\n",
    "args.d_ff = 2048  # dimension of fcn in model\n",
    "args.dropout = 0.05  # dropout\n",
    "args.attn = \"prob\"  # attention used in encoder, options:[prob, full]\n",
    "args.t_embed = \"timeF\"  # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = \"gelu\"  # activation\n",
    "args.distil = True  # whether to use distilling in encoder\n",
    "args.output_attention = False  # whether to output attention in encoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "\n",
    "args.seq_len = 64  # input sequence length of Informer encoder\n",
    "args.label_len = 32  # start token length of Informer decoder\n",
    "args.pred_len = 16  # prediction sequence length\n",
    "\n",
    "args.cols = [args.target, \"WTI_close\"]\n",
    "args.enc_in = 2  # encoder input size\n",
    "args.dec_in = 2  # decoder input size\n",
    "\n",
    "\n",
    "args.date_test = \"2022-04-01\"\n",
    "args.date_start = \"2021-01-01\"\n",
    "\n",
    "args.batch_size = 128\n",
    "args.learning_rate = 0.00001\n",
    "args.loss = \"mse\"\n",
    "args.lradj = \"type1\"\n",
    "args.use_amp = False  # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 3  # number of runs\n",
    "args.max_epochs = 10\n",
    "args.patience = 4\n",
    "args.des = \"assumption_leak\"\n",
    "\n",
    "args.scale = True\n",
    "args.inverse = True  # Defaultly False but @Zac thinks it should be True\n",
    "\n",
    "handle_gpu(args, None)\n",
    "\n",
    "# idk what this is for\n",
    "args.detail_freq = args.freq\n",
    "args.freq = args.freq[-1:]\n",
    "\n",
    "Exp = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data\n",
    "path = os.path.join(args.root_path, args.data_path)\n",
    "df = read_data(path)\n",
    "\n",
    "# Get target\n",
    "tick = args.target[: args.target.find(\"_\")]\n",
    "dat = args.target[args.target.find(\"_\") + 1 :]\n",
    "\n",
    "# Shift\n",
    "temp = df[tick, dat]\n",
    "temp = temp.shift(-1, fill_value=temp[-1])\n",
    "new_col_name = f\"{dat}shift\"\n",
    "df[tick, new_col_name] = temp\n",
    "df.sort_index(axis=1, inplace=True)\n",
    "new_col_name = f\"{tick}_{new_col_name}\"\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell once\n",
    "\n",
    "new_path = write_df(df, path, append=\"shift\")\n",
    "\n",
    "args.data_path = new_path[len(args.root_path) :]\n",
    "print(args.data_path)\n",
    "\n",
    "if args.cols is not None:\n",
    "    args.cols.append(new_col_name)\n",
    "\n",
    "args.enc_in += 1\n",
    "args.dec_in += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = None\n",
    "setting = None\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = setting_from_args(args, ii)\n",
    "\n",
    "    print(args)\n",
    "    # set experiments\n",
    "    exp = Exp(args)\n",
    "\n",
    "    # train\n",
    "    print(f\">>>>>>>start training : {setting}>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "    exp.train(setting)\n",
    "\n",
    "    # test\n",
    "    print(f\">>>>>>>testing : {setting}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "    exp.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct 24 2022, 16:07:47) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "44e5710a47a66ec240c2a0834fd7c20e15c61536e70be6891d892a39679ad994"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
