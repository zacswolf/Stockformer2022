{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers will pass the answer leaking test. \n",
    "Let's say there exists a column in the dataset that is the target moved back a time step so that it leaks the answer. Can the transformer based model find and us this information? This is literally a causality not a correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import dotdict\n",
    "from exp.exp_informer import Exp_Informer\n",
    "import torch\n",
    "from utils.ipynb_helpers import setting_from_args, read_data, write_df, handle_gpu\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "\n",
    "args.data = 'custom' # data\n",
    "args.root_path = './data/ETT/' # root path of data file\n",
    "\n",
    "\n",
    "args.data_path = 'close.csv' # data file\n",
    "args.features = 'MS' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'XOM_close' # target feature in S or MS task\n",
    "args.freq = 't' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.checkpoints = './checkpoints' # location of model checkpoints\n",
    "\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "args.c_out = 1 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.e_layers = 2 # num of encoder layers\n",
    "args.d_layers = 1 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in encoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "\n",
    "args.seq_len = 64 # input sequence length of Informer encoder\n",
    "args.label_len = 32 # start token length of Informer decoder\n",
    "args.pred_len = 16 # prediction sequence length\n",
    "\n",
    "args.cols = [args.target, \"WTI_close\"]\n",
    "args.enc_in = 2 # encoder input size\n",
    "args.dec_in = 2 # decoder input size\n",
    "\n",
    "\n",
    "args.date_test = \"2022-04-01\"\n",
    "args.date_cutoff = \"2021-01-01\"\n",
    "\n",
    "args.batch_size = 128 \n",
    "args.learning_rate = 0.00001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 3 # number of runs\n",
    "args.train_epochs = 10\n",
    "args.patience = 4\n",
    "args.des = 'assumption_leak'\n",
    "\n",
    "args.scale = True\n",
    "args.inverse = True # Defaultly False but @Zac thinks it should be True\n",
    "\n",
    "handle_gpu(args, None)\n",
    "\n",
    "# idk what this is for\n",
    "args.detail_freq = args.freq\n",
    "args.freq = args.freq[-1:]\n",
    "\n",
    "Exp = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data\n",
    "path = os.path.join(args.root_path, args.data_path)\n",
    "df = read_data(path)\n",
    "\n",
    "# Get target\n",
    "tick = args.target[:args.target.find(\"_\")]\n",
    "dat = args.target[args.target.find(\"_\")+1:]\n",
    "\n",
    "# Shift\n",
    "temp = df[tick, dat]\n",
    "temp = temp.shift(-1, fill_value=temp[-1])\n",
    "new_col_name = f\"{dat}shift\"\n",
    "df[tick, new_col_name] = temp\n",
    "df.sort_index(axis=1, inplace=True)\n",
    "new_col_name = f\"{tick}_{new_col_name}\"\n",
    "\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell once\n",
    "\n",
    "new_path = write_df(df, path, append=\"shift\")\n",
    "\n",
    "args.data_path = new_path[len(args.root_path):]\n",
    "print(args.data_path)\n",
    "\n",
    "if args.cols is not None:\n",
    "    args.cols.append(new_col_name)\n",
    "\n",
    "args.enc_in += 1\n",
    "args.dec_in += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = None\n",
    "setting = None\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = setting_from_args(args, ii)\n",
    "    \n",
    "    print(args)\n",
    "    # set experiments\n",
    "    exp = Exp(args)\n",
    "    \n",
    "    # train\n",
    "    print(f\">>>>>>>start training : {setting}>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "    exp.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print(f\">>>>>>>testing : {setting}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "    exp.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Aug 18 2022, 07:12:37) [GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "af42076dbd0dc4bad15096163af3221a89f84ba4155674ba19b5bb3ffd4e804c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
